{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hexiwear_HAR_training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5ZCsi_VFYyS"
      },
      "source": [
        "# Install TensorFlow 2.1.\n",
        "!pip install tensorflow==2.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaPp5xpqFgs5"
      },
      "source": [
        "# Import tensorflow as tf.\n",
        "import tensorflow as tf\n",
        "\n",
        "# Import numpy as np.\n",
        "import numpy as np\n",
        "\n",
        "# Import pandas as pd.\n",
        "import pandas as pd\n",
        "\n",
        "# Import matplotlib.\n",
        "import matplotlib\n",
        "\n",
        "# Import pyplot as plt.\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Import Model.\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "# Import Keras' various types of layer.\n",
        "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout\n",
        "\n",
        "# Import os.\n",
        "import os\n",
        "\n",
        "# Import drive.\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyGOjF9wGSGR"
      },
      "source": [
        "# Activities list.\n",
        "activities = ['Sitting', 'Lying', 'Standing', 'Jumping', 'Walking', 'Running']\n",
        "\n",
        "# Number of classes.\n",
        "N_CLASSES = len(activities)\n",
        "\n",
        "# Number of sensors.\n",
        "N_SENSORS = 6\n",
        "\n",
        "# Number of raw samples considered in a single inference.\n",
        "N_TIMESTEPS = 150"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COiDCUn0HbKl"
      },
      "source": [
        "# Function that reads the \"raw_data.csv\" file and returns a DataFrame.\n",
        "def read_data():\n",
        "\n",
        "  # Read data.\n",
        "  df = pd.read_csv('/content/drive/My Drive/raw_data.csv')\n",
        "\n",
        "  # Return df.\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VakSzstAHrez"
      },
      "source": [
        "# Function that pre-process the raw data.\n",
        "def preprocess(df):\n",
        "\n",
        "  # Cast df to numpy array.\n",
        "  data = df.to_numpy()\n",
        "\n",
        "  # Normalization of the accelerometer values.\n",
        "  data[:, :3] = data[:, :3] / 2.0\n",
        "\n",
        "  # Normalization of the gyroscore values.\n",
        "  data[:, 3:-1] = data[:, 3:-1] / 2000.0\n",
        "\n",
        "  # Reshape data.\n",
        "  data = data.reshape(int(len(data) / N_TIMESTEPS), N_TIMESTEPS, N_SENSORS + 1)\n",
        "\n",
        "  # Shuffle data.\n",
        "  np.random.shuffle(data)\n",
        "\n",
        "  # Return data.\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ezuvpzEuxag"
      },
      "source": [
        "# Function that allows the creation of the model.\n",
        "def get_model(FILTERS, KERNEL_SIZE, POOL_SIZE, NEURONS, DROPOUT_RATE):\n",
        "\n",
        "  # Creation of the model.\n",
        "  model = tf.keras.Sequential([\n",
        "    \n",
        "    # Convolutional layer.\n",
        "    Conv1D(filters = FILTERS, kernel_size = KERNEL_SIZE, activation = 'relu', input_shape = (N_TIMESTEPS, N_SENSORS), padding = 'same'),\n",
        "    \n",
        "    # Pooling layer.\n",
        "    MaxPooling1D(pool_size = POOL_SIZE, padding = 'same'),\n",
        "    \n",
        "    # Convolutional layer.\n",
        "    Conv1D(filters = FILTERS, kernel_size = KERNEL_SIZE, activation = 'relu', padding = 'same'),\n",
        "    \n",
        "    # Pooling layer.\n",
        "    MaxPooling1D(pool_size = POOL_SIZE, padding = 'same'),\n",
        "\n",
        "    # Convolutional layer.\n",
        "    Conv1D(filters = FILTERS, kernel_size = KERNEL_SIZE, activation = 'relu', padding = 'same'),\n",
        "    \n",
        "    # Pooling layer.\n",
        "    MaxPooling1D(pool_size = POOL_SIZE, padding = 'same'),\n",
        "    \n",
        "    # Dropout layer.\n",
        "    Dropout(DROPOUT_RATE),\n",
        "    \n",
        "    # Flatten layer.\n",
        "    Flatten(),\n",
        "    \n",
        "    # Fully connected layer.\n",
        "    Dense(NEURONS, activation = 'relu'),\n",
        "    \n",
        "    # Dropout layer.\n",
        "    Dropout(DROPOUT_RATE),\n",
        "    \n",
        "    # Fully connected layer.\n",
        "    Dense(N_CLASSES, activation = 'softmax')\n",
        "\n",
        "  ])\n",
        "\n",
        "  # Compile model.\n",
        "  model.compile(optimizer = 'adam', loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = False), metrics = ['accuracy'])\n",
        "\n",
        "  # Print the model informations.\n",
        "  model.summary()\n",
        "  \n",
        "  # Return model.\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Adrec0Fgx8c4"
      },
      "source": [
        "# Number of training epochs.\n",
        "EPOCHS = 100\n",
        "\n",
        "# Validation split.\n",
        "VALIDATION_SPLIT = 0.2\n",
        "\n",
        "# Number of kernels used by each convolutional layer.\n",
        "FILTERS  = 16\n",
        "\n",
        "# Kernel size.\n",
        "KERNEL_SIZE = 3\n",
        "\n",
        "# Pool size.\n",
        "POOL_SIZE = 3\n",
        "\n",
        "# Neurons in the single fully connected hidden layer.\n",
        "NEURONS = 128\n",
        "\n",
        "# Dropout rate.\n",
        "DROPOUT_RATE = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsgX8Y3SwUs_"
      },
      "source": [
        "# Read data and pre-process the entire dataset.\n",
        "data = preprocess(read_data())\n",
        "\n",
        "# Create the model.\n",
        "model = get_model(FILTERS, KERNEL_SIZE, POOL_SIZE, NEURONS, DROPOUT_RATE)\n",
        "\n",
        "# Train the model.\n",
        "model.fit(data[:, :, :N_SENSORS], data[:, 0, N_SENSORS], epochs = EPOCHS, validation_split = VALIDATION_SPLIT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkdNATYEy5-F"
      },
      "source": [
        "# Creazione dell'array contenente le features di validazione.\n",
        "test_samples = data[int((1 - VALIDATION_SPLIT) * len(data)):, :, :N_SENSORS]\n",
        "\n",
        "# Calibration steps.\n",
        "num_calibration_steps = 128\n",
        "\n",
        "# Calibration dtype.\n",
        "calibration_dtype = tf.float32\n",
        "\n",
        "# Function that allows the creation of a representative dataset in order to continue with the post-training quantization.\n",
        "def representative_dataset_gen():\n",
        "\n",
        "  # Cycle used to generate random samples.\n",
        "  for _ in range(num_calibration_steps):\n",
        "\n",
        "    # Random indexes generation.\n",
        "    rand_idx = np.random.randint(0, test_samples.shape[0] - 1)\n",
        "\n",
        "    # Pick samples according to the generated indexes.\n",
        "    sample = test_samples[rand_idx]\n",
        "\n",
        "    # Add new axis.\n",
        "    sample = sample[tf.newaxis, ...]\n",
        "\n",
        "    # Cast sample to calibration_dtype.\n",
        "    sample = tf.cast(sample, dtype = calibration_dtype)\n",
        "\n",
        "    # Yeld sample.\n",
        "    yield [sample]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4J11KLG0m-m"
      },
      "source": [
        "# Definition of the directory containing the models.\n",
        "MODELS_DIR = 'models/'\n",
        "\n",
        "# Check if \"models/\" already exists.\n",
        "if not os.path.exists(MODELS_DIR):\n",
        "\n",
        "    # Directory creation.\n",
        "    os.mkdir(MODELS_DIR)\n",
        "\n",
        "# Definition of the .tflite model's name.\n",
        "MODEL_TFLITE = MODELS_DIR + 'model.tflite'\n",
        "\n",
        "# Definition of the .cc model's name.\n",
        "MODEL_TFLITE_MICRO = MODELS_DIR + 'model.cc'\n",
        "\n",
        "# Create converter.\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "# Set optimizations.\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# Enforce full int8 quantization (except for inputs and outputs which are always floats)\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "\n",
        "# Provide a representative dataset.\n",
        "converter.representative_dataset = representative_dataset_gen\n",
        "\n",
        "# Convert the model.\n",
        "model_tflite = converter.convert()\n",
        "\n",
        "# Save the model.\n",
        "open(MODEL_TFLITE, 'wb').write(model_tflite)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-qoSNxI1Zxs"
      },
      "source": [
        "# xxd installation.\n",
        "!apt-get update && apt-get -qq install xxd\n",
        "\n",
        "# Model conversion.\n",
        "!xxd -i {MODEL_TFLITE} > {MODEL_TFLITE_MICRO}\n",
        "\n",
        "# Variables names' update.\n",
        "REPLACE_TEXT = MODEL_TFLITE.replace('/', '_').replace('.', '_')\n",
        "!sed -i 's/'{REPLACE_TEXT}'/g_model/g' {MODEL_TFLITE_MICRO}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyWFL-3a1nlB"
      },
      "source": [
        "# Print the .cc model.\n",
        "!cat {MODEL_TFLITE_MICRO}"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}